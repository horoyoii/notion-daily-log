#!/usr/bin/env python3
"""
ì§€ë‚œì£¼ ì—…ë¬´ë¡œê·¸ ì•„ì¹´ì´ë¸Œ ìŠ¤í¬ë¦½íŠ¸
ë§¤ì£¼ ê¸ˆìš”ì¼ 20ì‹œ(í•œêµ­ì‹œê°„) ì‹¤í–‰: ì§€ë‚œì£¼ ê¸ˆìš”ì¼ ì´ì „ì˜ ëª¨ë“  í˜ì´ì§€ë¥¼ ì•„ì¹´ì´ë¸Œ í˜ì´ì§€ë¡œ ì´ë™
í•˜ìœ„ í˜ì´ì§€ë„ ì¬ê·€ì ìœ¼ë¡œ ë³µì œí•©ë‹ˆë‹¤.
"""

import os
import sys
import requests
import json
import re
from datetime import datetime, timedelta
import logging
from typing import Optional
from dotenv import load_dotenv

# .env íŒŒì¼ ë¡œë“œ
load_dotenv()

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('archive_execution.log', encoding='utf-8'),
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger(__name__)


class NotionArchiver:
    """Notion ì—…ë¬´ë¡œê·¸ ì•„ì¹´ì´ë¸Œ"""

    def __init__(self, api_key: str, database_id: str, archive_page_id: str):
        self.api_key = api_key
        self.database_id = database_id
        self.archive_page_id = archive_page_id
        self.base_url = "https://api.notion.com/v1"
        self.headers = {
            "Authorization": f"Bearer {api_key}",
            "Content-Type": "application/json",
            "Notion-Version": "2022-06-28"
        }

    def get_pages_before_last_friday(self) -> list:
        """ì§€ë‚œì£¼ ê¸ˆìš”ì¼ ì´ì „ì˜ ëª¨ë“  í˜ì´ì§€ ì¡°íšŒ"""
        # í•œêµ­ ì‹œê°„ ê¸°ì¤€ ì˜¤ëŠ˜
        today = datetime.utcnow() + timedelta(hours=9)

        # ì§€ë‚œì£¼ ê¸ˆìš”ì¼ ê³„ì‚°
        days_since_friday = (today.weekday() - 4) % 7
        if days_since_friday == 0:
            # ì˜¤ëŠ˜ì´ ê¸ˆìš”ì¼ì´ë©´ ì§€ë‚œì£¼ ê¸ˆìš”ì¼ì€ 7ì¼ ì „
            last_friday = today - timedelta(days=7)
        else:
            # ì˜¤ëŠ˜ì´ ê¸ˆìš”ì¼ì´ ì•„ë‹ˆë©´ ê°€ì¥ ìµœê·¼ ê¸ˆìš”ì¼ ì°¾ê¸°
            last_friday = today - timedelta(days=days_since_friday)
        
        # ì§€ë‚œì£¼ ê¸ˆìš”ì¼ ì´ì „ = last_friday - timedelta(days=1) ì´ì „
        cutoff_date = last_friday - timedelta(days=1)
        cutoff_iso = cutoff_date.strftime('%Y-%m-%d')
        
        logger.info(f"ì§€ë‚œì£¼ ê¸ˆìš”ì¼: {last_friday.strftime('%Y-%m-%d')}")
        logger.info(f"ì•„ì¹´ì´ë¸Œ ëŒ€ìƒ: {cutoff_iso} ì´ì „ì˜ ëª¨ë“  í˜ì´ì§€")
        
        # ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ì‘ì„±ì¼ì´ cutoff_date ì´ì „ì¸ ëª¨ë“  í˜ì´ì§€ ì¡°íšŒ
        url = f"{self.base_url}/databases/{self.database_id}/query"
        all_pages = []
        has_more = True
        start_cursor = None
        
        while has_more:
            payload = {
                "filter": {
                    "and": [
                        {
                            "property": "ì‘ì„±ì¼",
                            "date": {
                                "before": cutoff_iso
                            }
                        }
                    ]
                }
            }
            
            if start_cursor:
                payload["start_cursor"] = start_cursor
            
            try:
                response = requests.post(url, headers=self.headers, json=payload)
                response.raise_for_status()
                
                data = response.json()
                results = data.get('results', [])
                
                # í˜ì´ì§€ ì œëª©ì´ ë‚ ì§œ í˜•ì‹ì¸ì§€ í™•ì¸í•˜ì—¬ ì—…ë¬´ë¡œê·¸ í˜ì´ì§€ë§Œ í•„í„°ë§
                for page in results:
                    title_property = page.get('properties', {}).get('ì´ë¦„', {})
                    title_array = title_property.get('title', [])
                    if title_array:
                        title = title_array[0].get('text', {}).get('content', '')
                        # ë‚ ì§œ í˜•ì‹ í™•ì¸: "YYYYë…„ MMì›” DDì¼ (ìš”ì¼)"
                        if re.match(r'\d{4}ë…„ \d{1,2}ì›” \d{1,2}ì¼ \([ì›”í™”ìˆ˜ëª©ê¸ˆí† ì¼]\)', title):
                            all_pages.append({
                                'id': page['id'],
                                'title': title,
                                'date': page.get('properties', {}).get('ì‘ì„±ì¼', {}).get('date', {}).get('start')
                            })
                
                has_more = data.get('has_more', False)
                start_cursor = data.get('next_cursor')
                
            except requests.exceptions.RequestException as e:
                logger.error(f"í˜ì´ì§€ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")
                if hasattr(e, 'response') and hasattr(e.response, 'text'):
                    logger.error(f"ì‘ë‹µ ë‚´ìš©: {e.response.text}")
                break
        
        logger.info(f"ì¡°íšŒëœ ì—…ë¬´ë¡œê·¸ í˜ì´ì§€: {len(all_pages)}ê°œ")
        return all_pages

    def get_korean_date_title(self, date: datetime) -> str:
        """í•œêµ­ ë‚ ì§œ í˜•ì‹ ì œëª© ë°˜í™˜"""
        weekday_names = ['ì›”', 'í™”', 'ìˆ˜', 'ëª©', 'ê¸ˆ', 'í† ', 'ì¼']
        weekday = weekday_names[date.weekday()]
        return f"{date.year}ë…„ {date.month}ì›” {date.day}ì¼ ({weekday})"

    def find_pages_by_dates(self, dates: list) -> list:
        """ë‚ ì§œ ëª©ë¡ì— í•´ë‹¹í•˜ëŠ” í˜ì´ì§€ë“¤ ì°¾ê¸°"""
        logger.info(f"í˜ì´ì§€ ê²€ìƒ‰ ì‹œì‘: {len(dates)}ì¼")

        found_pages = []

        for date in dates:
            title = self.get_korean_date_title(date)

            url = f"{self.base_url}/databases/{self.database_id}/query"
            payload = {
                "filter": {
                    "property": "ì´ë¦„",
                    "title": {
                        "equals": title
                    }
                }
            }

            try:
                response = requests.post(url, headers=self.headers, json=payload)
                response.raise_for_status()

                results = response.json().get('results', [])

                if results:
                    page = results[0]  # ì²« ë²ˆì§¸ ê²°ê³¼ë§Œ ì‚¬ìš©
                    found_pages.append({
                        'id': page['id'],
                        'title': title,
                        'date': date
                    })
                    logger.info(f"âœ… ë°œê²¬: {title}")
                else:
                    logger.warning(f"âš ï¸  ì—†ìŒ: {title}")

            except requests.exceptions.RequestException as e:
                logger.error(f"âŒ ê²€ìƒ‰ ì‹¤íŒ¨ ({title}): {str(e)}")

        return found_pages

    def get_page_blocks(self, page_id: str) -> list:
        """í˜ì´ì§€ì˜ ëª¨ë“  ë¸”ë¡ ê°€ì ¸ì˜¤ê¸° (í˜ì´ì§€ë„¤ì´ì…˜ ì²˜ë¦¬)"""
        url = f"{self.base_url}/blocks/{page_id}/children"
        all_blocks = []
        
        try:
            has_more = True
            start_cursor = None
            
            while has_more:
                params = {}
                if start_cursor:
                    params['start_cursor'] = start_cursor
                
                response = requests.get(url, headers=self.headers, params=params)
            response.raise_for_status()
                
                data = response.json()
                all_blocks.extend(data.get('results', []))
                has_more = data.get('has_more', False)
                start_cursor = data.get('next_cursor')
            
            return all_blocks
            
        except requests.exceptions.RequestException as e:
            logger.error(f"âŒ ë¸”ë¡ ì½˜í…ì¸  ì¡°íšŒ ì‹¤íŒ¨ ({page_id}): {e}")
            return []
    
    def get_block_children(self, block_id: str) -> Optional[list]:
        """í˜ì´ì§€ì˜ ëª¨ë“  í•˜ìœ„ ë¸”ë¡ì„ ê°€ì ¸ì˜µë‹ˆë‹¤. (í•˜ìœ„ í˜¸í™˜ì„± ìœ ì§€)"""
        blocks = self.get_page_blocks(block_id)
        return blocks if blocks else None

    def create_page(self, parent_id: str, title: str) -> Optional[str]:
        """ì§€ì •ëœ ë¶€ëª¨ ì•„ë˜ì— ìƒˆ í˜ì´ì§€ë¥¼ ë§Œë“­ë‹ˆë‹¤."""
        url = f"{self.base_url}/pages"
        payload = {
            "parent": {"page_id": parent_id},
            "properties": {
                "title": {
                    "title": [{"type": "text", "text": {"content": title}}]
                }
            }
        }
        try:
            response = requests.post(url, headers=self.headers, json=payload)
            response.raise_for_status()
            new_page_id = response.json()['id']
            logger.info(f"  ğŸ“„ ìƒˆ í˜ì´ì§€ ìƒì„± ì™„ë£Œ: {title} (ID: {new_page_id})")
            return new_page_id
        except requests.exceptions.RequestException as e:
            logger.error(f"âŒ ìƒˆ í˜ì´ì§€ ìƒì„± ì‹¤íŒ¨ ({title}): {e}")
            if hasattr(e.response, 'text'):
                logger.error(f"   ì‘ë‹µ: {e.response.text}")
            return None

    def append_block_children(self, block_id: str, children: list) -> bool:
        """í˜ì´ì§€ì— í•˜ìœ„ ë¸”ë¡ë“¤ì„ ì¶”ê°€í•©ë‹ˆë‹¤."""
        url = f"{self.base_url}/blocks/{block_id}/children"
        # APIëŠ” í•œ ë²ˆì— 100ê°œì˜ ë¸”ë¡ë§Œ ì¶”ê°€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
        for i in range(0, len(children), 100):
            chunk = children[i:i + 100]
            payload = {"children": chunk}
            try:
                response = requests.patch(url, headers=self.headers, json=payload)
                response.raise_for_status()
                logger.info(f"  â¡ï¸ ì½˜í…ì¸  ë¸”ë¡ {i+1}-{i+len(chunk)}/{len(children)} ì¶”ê°€ ì™„ë£Œ")
            except requests.exceptions.RequestException as e:
                logger.error(f"âŒ ì½˜í…ì¸  ì¶”ê°€ ì‹¤íŒ¨: {e}")
                if hasattr(e.response, 'text'):
                    response_text = e.response.text
                    logger.error(f"   ì‘ë‹µ: {response_text}")
                    # ì˜¤ë¥˜ ë©”ì‹œì§€ì—ì„œ ë¬¸ì œ ë¸”ë¡ì˜ ì¸ë±ìŠ¤ë¥¼ íŒŒì‹±í•©ë‹ˆë‹¤.
                    try:
                        match = re.search(r"body\.children\[(\d+)\]", response_text)
                        if match:
                            problem_index = int(match.group(1))
                            problem_block = payload["children"][problem_index]
                            logger.error(f"   ğŸš¨ ë¬¸ì œê°€ ë°œìƒí•œ ë¸”ë¡ (ì¸ë±ìŠ¤ {problem_index}):")
                            logger.error(json.dumps(problem_block, indent=2, ensure_ascii=False))
                    except Exception as parse_error:
                        logger.error(f"   (ì˜¤ë¥˜ ë©”ì‹œì§€ íŒŒì‹± ì‹¤íŒ¨: {parse_error})")
                return False
        return True

    def delete_page(self, page_id: str, page_title: str) -> bool:
        """í˜ì´ì§€ë¥¼ ë³´ê´€ ì²˜ë¦¬í•˜ì—¬ ì‚­ì œí•©ë‹ˆë‹¤."""
        url = f"{self.base_url}/pages/{page_id}"
        payload = {"archived": True}
        try:
            response = requests.patch(url, headers=self.headers, json=payload)
            response.raise_for_status()
            logger.info(f"  ğŸ—‘ï¸ ì›ë³¸ í˜ì´ì§€ ë³´ê´€ ì™„ë£Œ: {page_title}")
            return True
        except requests.exceptions.RequestException as e:
            logger.error(f"âŒ ì›ë³¸ í˜ì´ì§€ ë³´ê´€ ì‹¤íŒ¨ ({page_title}): {e}")
            return False

    def clean_block_for_copy(self, block: dict) -> dict:
        """ë¸”ë¡ ë°ì´í„°ë¥¼ ë³µì‚¬ ê°€ëŠ¥í•œ í˜•íƒœë¡œ ì •ë¦¬"""
        block_type = block.get('type')
        if not block_type:
            return None
        
        # child_page, child_databaseëŠ” ë³„ë„ë¡œ ì²˜ë¦¬í•˜ë¯€ë¡œ ì œì™¸
        if block_type in ['child_page', 'child_database']:
            return None
        
        # ë³µì‚¬í•  ìˆ˜ ì—†ëŠ” ë¸”ë¡ íƒ€ì…ë“¤
        unsupported_blocks = ['link_preview', 'unsupported']
        if block_type in unsupported_blocks:
            logger.warning(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ë¸”ë¡ íƒ€ì…: {block_type}")
            return None
        
        # ê¸°ë³¸ ë¸”ë¡ êµ¬ì¡°
        cleaned_block = {
            'type': block_type,
            block_type: {}
        }
        
        # ë¸”ë¡ íƒ€ì…ë³„ ë°ì´í„° ë³µì‚¬
        original_content = block.get(block_type, {})
        
        # ë¹ˆ ë¸”ë¡ íƒ€ì… (divider, breadcrumb, table_of_contents ë“±)
        empty_block_types = ['divider', 'breadcrumb', 'table_of_contents']
        if block_type in empty_block_types:
            return cleaned_block
        
        # rich_textê°€ ìˆëŠ” ê²½ìš° ë³µì‚¬
        if 'rich_text' in original_content:
            cleaned_block[block_type]['rich_text'] = original_content['rich_text']
        
        # ë‹¤ë¥¸ ì†ì„±ë“¤ë„ ë³µì‚¬ (read-only í•„ë“œëŠ” ì œì™¸)
        readonly_fields = ['id', 'created_time', 'last_edited_time', 'created_by', 'last_edited_by', 'has_children', 'archived', 'parent']
        for key, value in original_content.items():
            if key not in readonly_fields and key not in cleaned_block[block_type]:
                cleaned_block[block_type][key] = value
        
        return cleaned_block

    def _clean_block_for_append(self, block: dict) -> dict:
        """APIë¡œ ë¸”ë¡ì„ ë‹¤ì‹œ ë³´ë‚¼ ë•Œ í•„ìš”í•œ í‚¤ë§Œ í¬í•¨í•˜ëŠ” ìƒˆ ê°ì²´ë¥¼ ë§Œë“­ë‹ˆë‹¤. (í•˜ìœ„ í˜¸í™˜ì„± ìœ ì§€)"""
        cleaned = self.clean_block_for_copy(block)
        if cleaned:
            return cleaned
        
        # ê¸°ì¡´ ë¡œì§ ìœ ì§€
        block_type = block.get("type")
        if block_type and block.get(block_type):
            return {
                "type": block_type,
                block_type: block[block_type]
            }
        
        block_copy = block.copy()
        block_copy.pop('id', None)
        block_copy.pop('parent', None)
        block_copy.pop('created_time', None)
        block_copy.pop('last_edited_time', None)
        block_copy.pop('created_by', None)
        block_copy.pop('last_edited_by', None)
        block_copy.pop('has_children', None)
        block_copy.pop('object', None)
        return block_copy
    
    def get_child_pages(self, page_id: str) -> list:
        """í˜ì´ì§€ì˜ ëª¨ë“  í•˜ìœ„ í˜ì´ì§€ ê°€ì ¸ì˜¤ê¸°"""
        url = f"{self.base_url}/blocks/{page_id}/children"
        child_pages = []
        
        try:
            has_more = True
            start_cursor = None
            
            while has_more:
                params = {}
                if start_cursor:
                    params['start_cursor'] = start_cursor
                
                response = requests.get(url, headers=self.headers, params=params)
                response.raise_for_status()
                
                data = response.json()
                blocks = data.get('results', [])
                
                # child_page íƒ€ì…ì˜ ë¸”ë¡ë§Œ í•„í„°ë§
                for block in blocks:
                    if block.get('type') == 'child_page':
                        child_pages.append(block)
                
                has_more = data.get('has_more', False)
                start_cursor = data.get('next_cursor')
            
            return child_pages
            
        except requests.exceptions.RequestException as e:
            logger.error(f"í•˜ìœ„ í˜ì´ì§€ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")
            return []
    
    def copy_block_children(self, source_block_id: str, target_block_id: str):
        """ë¸”ë¡ì˜ ìì‹ ë¸”ë¡ë“¤ì„ ì¬ê·€ì ìœ¼ë¡œ ë³µì‚¬"""
        child_blocks = self.get_page_blocks(source_block_id)
        if not child_blocks:
            return
        self.copy_blocks_to_page(target_block_id, child_blocks)
    
    def copy_blocks_to_page(self, target_page_id: str, blocks: list):
        """ë¸”ë¡ë“¤ì„ ëŒ€ìƒ í˜ì´ì§€ë¡œ ìˆœì„œëŒ€ë¡œ ë³µì‚¬ (ì¼ë°˜ ë¸”ë¡ + child_page í¬í•¨)"""
        if not blocks:
            return
        
        import time
        logger.info(f"  ë¸”ë¡ ë³µì‚¬ ì‹œì‘: {len(blocks)}ê°œ (ìˆœì„œ ìœ ì§€)")
        
        for block in blocks:
            block_type = block.get('type')
            
            # child_pageëŠ” ë³„ë„ë¡œ ì²˜ë¦¬
            if block_type == 'child_page':
                child_title = block.get('child_page', {}).get('title', 'ì œëª© ì—†ìŒ')
                logger.info(f"  í•˜ìœ„ í˜ì´ì§€ ë°œê²¬ (ìˆœì„œ ìœ ì§€): {child_title}")
                time.sleep(0.5)
                try:
                    self.copy_child_page_recursive(block['id'], target_page_id)
                except Exception as e:
                    logger.error(f"  í•˜ìœ„ í˜ì´ì§€ ë³µì‚¬ ì‹¤íŒ¨: {str(e)}")
                continue
            
            # child_databaseëŠ” ìŠ¤í‚µ
            if block_type == 'child_database':
                logger.warning(f"  child_databaseëŠ” í˜„ì¬ ì§€ì›í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {block['id']}")
                continue
            
            # ì¼ë°˜ ë¸”ë¡ ë³µì‚¬
            cleaned_block = self.clean_block_for_copy(block)
            if not cleaned_block:
                continue
            
            url = f"{self.base_url}/blocks/{target_page_id}/children"
            payload = {"children": [cleaned_block]}
            
            try:
                response = requests.patch(url, headers=self.headers, json=payload)
                response.raise_for_status()
                
                result = response.json()
                created_blocks = result.get('results', [])
                
                if created_blocks:
                    created_block = created_blocks[0]
                    
                    # ìì‹ ë¸”ë¡ì´ ìˆëŠ” ê²½ìš° ì¬ê·€ì ìœ¼ë¡œ ë³µì‚¬
                    if block.get('has_children'):
                        original_block_id = block['id']
                        created_block_id = created_block['id']
                        time.sleep(0.3)
                        try:
                            self.copy_block_children(original_block_id, created_block_id)
                        except Exception as e:
                            logger.error(f"  ì¤‘ì²© ë¸”ë¡ ë³µì‚¬ ì‹¤íŒ¨: {str(e)}")
                
                time.sleep(0.3)
                
            except requests.exceptions.RequestException as e:
                logger.error(f"  ë¸”ë¡ ë³µì‚¬ ì‹¤íŒ¨ ({block_type}): {str(e)}")
    
    def copy_child_page_recursive(self, source_page_id: str, target_parent_id: str):
        """í•˜ìœ„ í˜ì´ì§€ë¥¼ ì¬ê·€ì ìœ¼ë¡œ ë³µì‚¬"""
        import time
        
        try:
            # 1. ì›ë³¸ í˜ì´ì§€ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
            url = f"{self.base_url}/pages/{source_page_id}"
            response = requests.get(url, headers=self.headers)
            response.raise_for_status()
            
            source_page = response.json()
            
            # 2. í˜ì´ì§€ ì œëª© ì¶”ì¶œ
            title_property = source_page.get('properties', {}).get('title', {})
            title_array = title_property.get('title', [])
            title = title_array[0].get('text', {}).get('content', 'ì œëª© ì—†ìŒ') if title_array else 'ì œëª© ì—†ìŒ'
            
            logger.info(f"    í•˜ìœ„ í˜ì´ì§€ ë³µì‚¬ ì‹œì‘: {title}")
            
            # 3. ìƒˆ í•˜ìœ„ í˜ì´ì§€ ìƒì„±
            new_page_id = self.create_page(target_parent_id, title)
            if not new_page_id:
                return
            
            time.sleep(0.5)
            
            # 4. ì›ë³¸ í˜ì´ì§€ì˜ ë¸”ë¡ ë³µì‚¬
            source_blocks = self.get_page_blocks(source_page_id)
            if source_blocks:
                self.copy_blocks_to_page(new_page_id, source_blocks)
            
            time.sleep(0.5)
            
            # 5. í•˜ìœ„ í˜ì´ì§€ì˜ í•˜ìœ„ í˜ì´ì§€ë“¤ë„ ì¬ê·€ì ìœ¼ë¡œ ë³µì‚¬
            child_pages = self.get_child_pages(source_page_id)
            for child_page in child_pages:
                child_page_id = child_page['id']
                time.sleep(0.5)
                self.copy_child_page_recursive(child_page_id, new_page_id)
            
            logger.info(f"    í•˜ìœ„ í˜ì´ì§€ ë³µì‚¬ ì™„ë£Œ: {title}")
            
        except Exception as e:
            logger.error(f"    í•˜ìœ„ í˜ì´ì§€ ë³µì‚¬ ì‹¤íŒ¨: {str(e)}")

    def move_page(self, page_id: str, page_title: str) -> bool:
        """í˜ì´ì§€ë¥¼ ì½ê³ , ìƒˆë¡œ ë§Œë“¤ê³ , ë³µì‚¬í•œ ë’¤ ì›ë³¸ì„ ì‚­ì œí•©ë‹ˆë‹¤. (í•˜ìœ„ í˜ì´ì§€ ì¬ê·€ ë³µì œ í¬í•¨)"""
        import time
        logger.info(f"í˜ì´ì§€ ì´ë™ ì‹œì‘: {page_title}")

        # 1. ì›ë³¸ í˜ì´ì§€ì˜ ì½˜í…ì¸ ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
        content_blocks = self.get_page_blocks(page_id)
        logger.info(f"  ğŸ“š ì›ë³¸ ì½˜í…ì¸  {len(content_blocks)}ê°œ ë¸”ë¡ ì½ê¸° ì™„ë£Œ")

        # 2. 'ì•„ì¹´ì´ë¸Œ' í˜ì´ì§€ ì•„ë˜ì— ìƒˆ í˜ì´ì§€ë¥¼ ë§Œë“­ë‹ˆë‹¤.
        new_page_id = self.create_page(self.archive_page_id, page_title)
        if not new_page_id:
            return False

        time.sleep(0.5)

        # 3. ìƒˆ í˜ì´ì§€ì— ì½˜í…ì¸ ë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤. (ìˆœì„œ ìœ ì§€, í•˜ìœ„ í˜ì´ì§€ í¬í•¨)
        if content_blocks:
            self.copy_blocks_to_page(new_page_id, content_blocks)
        logger.info(f"  âœ… ì½˜í…ì¸  ë³µì‚¬ ì™„ë£Œ")

        # 4. í•˜ìœ„ í˜ì´ì§€ë„ ì¬ê·€ì ìœ¼ë¡œ ë³µì‚¬
        child_pages = self.get_child_pages(page_id)
        if child_pages:
            logger.info(f"  ğŸ“ í•˜ìœ„ í˜ì´ì§€ {len(child_pages)}ê°œ ë°œê²¬, ì¬ê·€ ë³µì œ ì‹œì‘")
            for child_page in child_pages:
                child_page_id = child_page['id']
                time.sleep(0.5)
                self.copy_child_page_recursive(child_page_id, new_page_id)
            logger.info(f"  âœ… í•˜ìœ„ í˜ì´ì§€ ë³µì œ ì™„ë£Œ")

        # 5. ì›ë³¸ í˜ì´ì§€ë¥¼ ì‚­ì œí•©ë‹ˆë‹¤.
        if not self.delete_page(page_id, page_title):
            logger.error(f"!! ì›ë³¸ í˜ì´ì§€({page_id}) ì‚­ì œ ì‹¤íŒ¨. ìˆ˜ë™ í™•ì¸ì´ í•„ìš”í•©ë‹ˆë‹¤.")
            return False
        
        logger.info(f"âœ… ì´ë™ ì™„ë£Œ: {page_title}")
        return True

    def archive_last_week(self):
        """ì§€ë‚œì£¼ ê¸ˆìš”ì¼ ì´ì „ì˜ ëª¨ë“  í˜ì´ì§€ ì•„ì¹´ì´ë¸Œ"""
        logger.info("=" * 80)
        logger.info("ì§€ë‚œì£¼ ê¸ˆìš”ì¼ ì´ì „ ì—…ë¬´ë¡œê·¸ ì•„ì¹´ì´ë¸Œ ì‹œì‘")
        logger.info("=" * 80)

        # 1. ì§€ë‚œì£¼ ê¸ˆìš”ì¼ ì´ì „ì˜ ëª¨ë“  í˜ì´ì§€ ì¡°íšŒ
        pages = self.get_pages_before_last_friday()

        logger.info(f"\në°œê²¬ëœ í˜ì´ì§€: {len(pages)}ê°œ")

        if not pages:
            logger.info("ì•„ì¹´ì´ë¸Œí•  í˜ì´ì§€ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return

        # 2. í˜ì´ì§€ ì´ë™
        logger.info(f"\nì•„ì¹´ì´ë¸Œ í˜ì´ì§€ë¡œ ì´ë™ ì‹œì‘:")
        logger.info(f"ëŒ€ìƒ: https://www.notion.so/{self.archive_page_id.replace('-', '')}\n")

        success_count = 0
        fail_count = 0

        # ë‚ ì§œ ì—­ìˆœìœ¼ë¡œ ì´ë™ (ìµœì‹ ì´ ìœ„ë¡œ ì˜¤ë„ë¡)
        pages_sorted = sorted(pages, key=lambda x: x.get('date', ''), reverse=True)
        for page in pages_sorted:
            import time
            time.sleep(1)  # API ì†ë„ ì œí•œ ë°©ì§€

            if self.move_page(page['id'], page['title']):
                success_count += 1
            else:
                fail_count += 1
                logger.error(f"ğŸ”¥ ì „ì²´ ì´ë™ ì‹¤íŒ¨: {page['title']}. ë‹¤ìŒ í˜ì´ì§€ë¡œ ê³„ì† ì§„í–‰í•©ë‹ˆë‹¤.")

        # 3. ê²°ê³¼ ìš”ì•½
        logger.info("\n" + "=" * 80)
        logger.info("ì•„ì¹´ì´ë¸Œ ì™„ë£Œ")
        logger.info("=" * 80)
        logger.info(f"ì„±ê³µ: {success_count}ê°œ")
        logger.info(f"ì‹¤íŒ¨: {fail_count}ê°œ")
        logger.info(f"ì „ì²´: {len(pages)}ê°œ")


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    # í™˜ê²½ë³€ìˆ˜ì—ì„œ ì„¤ì • ë¡œë“œ
    api_key = os.getenv('NOTION_API_KEY')
    database_id = os.getenv('DATA_SOURCE_ID')
    archive_page_id = os.getenv('ARCHIVE_PAGE_ID', '1cb5aae782eb807c81cef3bd6e2345ee')

    # í•„ìˆ˜ í™˜ê²½ë³€ìˆ˜ í™•ì¸
    if not all([api_key, database_id]):
        logger.error("í•„ìˆ˜ í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        logger.error("NOTION_API_KEY, DATA_SOURCE_IDë¥¼ í™•ì¸í•˜ì„¸ìš”.")
        sys.exit(1)

    # ì•„ì¹´ì´ë¸Œ ì‹¤í–‰
    archiver = NotionArchiver(api_key, database_id, archive_page_id)
    archiver.archive_last_week()


if __name__ == "__main__":
    main()
